# Telkom Paper Research API - Backend

FastAPI + DSPy backend for AI-powered paper research platform.

## ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py              # Package init
â”‚   â”œâ”€â”€ main.py                  # FastAPI app entry point
â”‚   â”œâ”€â”€ config.py                # Settings & configuration
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ chat.py          # AI chat endpoints
â”‚   â”‚       â”œâ”€â”€ papers.py        # Paper search endpoints
â”‚   â”‚       â””â”€â”€ health.py        # Health check endpoints
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ models.py            # Pydantic schemas
â”‚   â”‚   â””â”€â”€ exceptions.py        # Custom exceptions
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rag.py               # RAG service with DSPy
â”‚   â”‚   â””â”€â”€ retriever.py         # Paper retriever
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ streaming.py         # SSE streaming utilities
â”œâ”€â”€ data/
â”‚   â””â”€â”€ papers.json              # Paper data (optional)
â”œâ”€â”€ .env                         # Environment variables
â”œâ”€â”€ .env.example                 # Environment template
â”œâ”€â”€ requirements.txt             # Dependencies
â””â”€â”€ run.py                       # Convenience runner
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
cd backend
pip install -r requirements.txt
```

### 2. Configure Environment

```bash
cp .env.example .env
# Edit .env and add your API keys
```

### 3. Run the Server

```bash
# Activate virtual environment
.venv\Scripts\activate

# Option 1: Using run.py
python run.py

# Option 2: Using uvicorn directly
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Option 3: Using main.py
python -m app.main
```

## ğŸ“¡ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | API info & available endpoints |
| `/health` | GET | Health check |
| `/papers/search` | GET/POST | Search papers by keyword |
| `/papers/list` | GET | List all papers |
| `/chat/basic` | POST | AI chat (streaming or sync) |
| `/chat/deep` | POST | Deep research (RLM) - TBD |

## ğŸ”§ Configuration

All configuration is done via environment variables (see `.env.example`):

| Variable | Required | Description |
|----------|----------|-------------|
| `OPENROUTER_API_KEY` | Yes* | OpenRouter API key |
| `OPENAI_API_KEY` | Yes* | OpenAI API key (alternative) |
| `DSPY_MODEL` | No | Model to use (default: Gemini Pro) |
| `DSPY_MAX_WORKERS` | No | Async workers (default: 4) |
| `RETRIEVAL_TOP_K` | No | Papers per query (default: 3) |
| `REDIS_URL` | No** | Redis URL for session management |
| `SESSION_TTL` | No | Session timeout (default: 3600s) |
| `DATABASE_URL` | No | PostgreSQL/Supabase for long-term storage |

*At least one API key is required.  
**Required for conversation history feature.

## ğŸ§ª Testing

### New API Structure (Recommended)

```bash
# Health check
curl http://localhost:8000/health

# Search papers
curl "http://localhost:8000/papers/search?query=machine+learning&limit=5"

# AI chat with new meta_params structure
curl -X POST http://localhost:8000/chat/basic \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is deep learning?",
    "meta_params": {
      "stream": false,
      "language": "id-ID",
      "source_preference": "all"
    }
  }'

# Streaming with conversation history
curl -X POST http://localhost:8000/chat/basic \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Tell me more about that",
    "meta_params": {
      "stream": true,
      "conversation_id": "conv_abc123"
    }
  }'
```

### Backwards Compatible (Old Format Still Works)

```bash
# Old format still supported
curl -X POST http://localhost:8000/chat/basic \
  -H "Content-Type: application/json" \
  -d '{"question": "What is deep learning?", "stream": false}'
```

## ğŸ“š Architecture

### RAG Flow

```
User Question + Conversation History
     â†“
Intent Classification (research vs general)
     â†“
Query Generation (LLM-optimized search keywords)
     â†“
PaperRetriever (vector/keyword search)
     â†“
RAG Module (DSPy + ChainOfThought + History)
     â†“
Streaming Response (SSE) or JSON
```

### Services

- **PaperRetriever**: Simple keyword-based search (replaceable with vector search)
- **RAGService**: DSPy module for question answering with citations
- **Streaming Utils**: SSE formatting for real-time responses

## ğŸ†• New API Structure

We've upgraded to a cleaner API structure with `meta_params`:

```json
{
  "query": "What is machine learning?",
  "meta_params": {
    "mode": "basic",
    "stream": true,
    "language": "id-ID",
    "timezone": "Asia/Jakarta",
    "source_preference": "all",
    "conversation_id": "conv_123",
    "is_incognito": false,
    "attachments": []
  }
}
```

### New Features:
- ğŸŒ **Multilingual**: Respond in Indonesian, English, etc.
- ğŸ“š **Source Filtering**: Papers only, general knowledge, or all
- ğŸ•µï¸ **Incognito Mode**: Private queries (no history saved)
- ğŸŒ **Timezone Aware**: Context-aware timestamps
- ğŸ“ **Attachments**: File upload support (coming soon)

**Migration:** See [`docs/API_MIGRATION_GUIDE.md`](docs/API_MIGRATION_GUIDE.md)

## ğŸ’¬ Conversation History & Session Management

The agent supports multi-turn conversations with Redis-based session management!

### Quick Start:

```bash
# 1. Start Redis (required for conversation history)
docker run -d -p 6379:6379 redis:alpine

# 2. First message (creates new conversation)
curl -X POST http://localhost:8000/chat/basic \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What papers discuss transformers?",
    "meta_params": {"stream": false}
  }'

# 3. Follow-up (agent remembers context via Redis)
curl -X POST http://localhost:8000/chat/basic \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Which one is most recent?",
    "meta_params": {
      "stream": false,
      "conversation_id": "conv_abc123"
    }
  }'
```

### Architecture:

**Hybrid Redis + Database** (recommended for production)
- ğŸš€ Redis: Fast access (<5ms) for active conversations
- ğŸ’¾ Database: Long-term storage and analytics
- ğŸ§¹ Auto-cleanup: TTL-based session expiry
- ğŸ“Š Scalable: Handles 10k+ concurrent users

### Features:

- âœ… Context-aware responses using `dspy.History`
- âœ… Redis-based session management (fast!)
- âœ… Optional database sync for durability
- âœ… Automatic history pruning
- âœ… Cross-device conversation continuity
- âœ… Incognito mode for privacy

### Resources:

- **[Architecture Guide](docs/ARCHITECTURE_CONVERSATION_HISTORY.md)** â­ Start here!
- **[API Migration Guide](docs/API_MIGRATION_GUIDE.md)** - Update your code
- **[Session Manager](app/services/session_manager.py)** - Implementation
- **[Examples](examples/conversation_with_session.py)** - Interactive demos

## ğŸ”® Future Improvements

1. **Vector Search**: Replace keyword search with `dspy.retrievers.Embeddings`
2. **Session Persistence**: Add Redis/DB for persistent conversation storage
3. **RLM Agent**: Implement recursive language model for deep research
4. **Authentication**: Add JWT-based auth
5. **Rate Limiting**: Protect API endpoints
6. **History Summarization**: Auto-summarize long conversations

## ğŸ“– Documentation

- See `docs/` for detailed implementation guides
- FastAPI docs: http://localhost:8000/docs (auto-generated)
- OpenAPI spec: http://localhost:8000/openapi.json
